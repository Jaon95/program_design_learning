{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 手写数字识别实践\n",
    "\n",
    "本实践将结合课程关于深度学习的理论知识，使用python语言和深度学习框架paddlepaddle构建全连接网络多次感知器，用于预测手写数字图片。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ba26eac845334208851e72c7a2dfef5e1eec566894ba430aba7492e72c49cacd)\n",
    "\n",
    "## 背景介绍\n",
    "当我们学习编程的时候，编写的第一个程序一般是实现打印\"Hello World\"。而机器学习（或深度学习）的入门教程，一般都是 [MNIST](http://yann.lecun.com/exdb/mnist/) 数据库上的手写识别问题。原因是手写识别属于典型的图像分类问题，比较简单，同时MNIST数据集也很完备。MNIST数据集作为一个简单的计算机视觉数据集，包含一系列如图1所示的手写数字图片和对应的标签。图片是28x28的像素矩阵，标签则对应着0~9的10个数字。每张图片都经过了大小归一化和居中处理。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai.bdstatic.com/file/C1EBD529CA9442D7956E20FA221C32D1\" width=\"400\"><br/>\n",
    "图1. MNIST图片示例\n",
    "</p>\n",
    "\n",
    "MNIST数据集是从 [NIST](https://www.nist.gov/srd/nist-special-database-19) 的Special Database 3（SD-3）和Special Database 1（SD-1）构建而来。由于SD-3是由美国人口调查局的员工进行标注，SD-1是由美国高中生进行标注，因此SD-3比SD-1更干净也更容易识别。Yann LeCun等人从SD-1和SD-3中各取一半作为MNIST的训练集（60000条数据）和测试集（10000条数据），其中训练集来自250位不同的标注员，此外还保证了训练集和测试集的标注员是不完全相同的。\n",
    "\n",
    "Yann LeCun早先在手写字符识别上做了很多研究，并在研究过程中提出了卷积神经网络（Convolutional Neural Network），大幅度地提高了手写字符的识别能力，也因此成为了深度学习领域的奠基人之一。如今的深度学习领域，卷积神经网络占据了至关重要的地位，从最早Yann LeCun提出的简单LeNet，到如今ImageNet大赛上的优胜模型VGGNet、GoogLeNet、ResNet等（请参见[图像分类](https://github.com/PaddlePaddle/book/tree/develop/03.image_classification) 教程），人们在图像分类领域，利用卷积神经网络得到了一系列惊人的结果。\n",
    "\n",
    "有很多算法在MNIST上进行实验。1998年，LeCun分别用单层线性分类器、多层感知器（Multilayer Perceptron, MLP）和多层卷积神经网络LeNet进行实验，使得测试集上的误差不断下降（从12%下降到0.7%）\\[[1](#参考文献)\\]。此后，科学家们又基于K近邻（K-Nearest Neighbors）算法\\[[2](#参考文献)\\]、支持向量机（SVM）\\[[3](#参考文献)\\]、神经网络\\[[4-7](#参考文献)\\]和Boosting方法\\[[8](#参考文献)\\]等做了大量实验，并采用多种预处理方法（如去除歪曲、去噪、模糊等）来提高识别的准确率。\n",
    "\n",
    "本教程中，我们从简单的模型Softmax回归开始，带大家入门手写字符识别，并逐步进行模型优化。\n",
    "\n",
    "\n",
    "## 模型概览\n",
    "\n",
    "基于MNIST数据训练一个分类器，在介绍本教程使用的三个基本图像分类网络前，我们先给出一些定义：\n",
    "- $X$是输入：MNIST图片是$28\\times28$ 的二维图像，为了进行计算，我们将其转化为$784$维向量，即𝑋=(𝑥0,𝑥1,…,𝑥783)。\n",
    "- $Y$是输出：分类器的输出是10类数字（0-9），即 𝑌=(𝑦0,𝑦1,…,𝑦9)，每一维$y_i$代表图片分类为第$i$类数字的概率。\n",
    "- $L$是图片的真实标签：𝐿=(𝑙0,𝑙1,…,𝑙9)也是10维，但只有一维为1，其他都为0。\n",
    "\n",
    "### Softmax回归(Softmax Regression)\n",
    "\n",
    "最简单的Softmax回归模型是先将输入层经过一个全连接层得到的特征，然后直接通过softmax 函数进行多分类\\[[9](#参考文献)\\]。\n",
    "\n",
    "输入层的数据$X$传到输出层，在激活操作之前，会乘以相应的权重 $W$ ，并加上偏置变量 $b$ ，具体如下：\n",
    "\n",
    "$$ y_i = \\text{softmax}(\\sum_j W_{i,j}x_j + b_i) $$\n",
    "\n",
    "其中：<p align=\"center\">![](https://ai-studio-static-online.cdn.bcebos.com/4a47b0324d4646fda25716a8cce61275e8f7215424014fae8ac1a22b6840ea57)</p>\n",
    "\n",
    "对于有 $N$ 个类别的多分类问题，指定 $N$ 个输出节点，$N$ 维结果向量经过softmax将归一化为 $N$ 个[0,1]范围内的实数值，分别表示该样本属于这 $N$ 个类别的概率。此处的 $y_i$ 即对应该图片为数字 $i$ 的预测概率。\n",
    "\n",
    "在分类问题中，我们一般采用交叉熵代价损失函数（cross entropy），公式如下：\n",
    "\n",
    "$$  \\text{crossentropy}(label, y) = -\\sum_i label_ilog(y_i) $$\n",
    "\n",
    "图2为softmax回归的网络图，图中权重用蓝线表示、偏置用红线表示、+1代表偏置参数的系数为1。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai.bdstatic.com/file/7C68861433494EDA9FCA8369FAF2D3C3\" width=400><br/>\n",
    "图2. softmax回归网络结构图<br/>\n",
    "</p>\n",
    "\n",
    "### 多层感知器(Multilayer Perceptron, MLP)\n",
    "\n",
    "Softmax回归模型采用了最简单的两层神经网络，即只有输入层和输出层，因此其拟合能力有限。为了达到更好的识别效果，我们考虑在输入层和输出层中间加上若干个隐藏层\\[[10](#参考文献)\\]。\n",
    "\n",
    "1.  经过第一个隐藏层，可以得到 $H_1 = \\phi(W_1X + b_1)$，其中$\\phi$代表激活函数，常见的有sigmoid、tanh或ReLU等函数。\n",
    "2.  经过第二个隐藏层，可以得到 $H_2 = \\phi(W_2H_1 + b_2)$。\n",
    "3.  最后，再经过输出层，得到的$Y=\\text{softmax}(W_3H_2 + b_3)$，即为最后的分类结果向量。\n",
    "\n",
    "\n",
    "图3为多层感知器的网络结构图，图中权重用蓝线表示、偏置用红线表示、+1代表偏置参数的系数为1。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai.bdstatic.com/file/75D4C45A944748B7BF56F4DD92D5B6FB\" width=500><br/>\n",
    "图3. 多层感知器网络结构图<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 实践过程\n",
    " \n",
    " 实践总体过程和步骤如下图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/daa656fb63c54dec8e0e468ba8f65e1c94ccd7debd314d8ca6d179f12f13883d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "首先导入必要的包\n",
    "\n",
    "numpy---------->python第三方库，用于进行科学计算\n",
    "\n",
    "PIL------------> Python Image Library,python第三方图像处理库\n",
    "\n",
    "matplotlib----->python的绘图库 pyplot:matplotlib的绘图框架\n",
    "\n",
    "os------------->提供了丰富的方法来处理文件和目录\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#导入需要的包\n",
    "import numpy  as np\n",
    "import paddle as paddle\n",
    "import paddle.fluid as fluid\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Step1：准备数据。**\n",
    "\n",
    "(1)数据集介绍\n",
    "\n",
    "MNIST数据集包含60000个训练集和10000测试数据集。分为图片和标签，图片是28*28的像素矩阵，标签为0~9共10个数字。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fc73217ae57f451a89badc801a903bb742e42eabd9434ecc8089efe19a66c076)\n",
    "\n",
    "(2)train_reader和test_reader\n",
    "\n",
    "paddle.dataset.mnist.train()和test()分别用于获取mnist训练集和测试集\n",
    "\n",
    "paddle.reader.shuffle()表示每次缓存BUF_SIZE个数据项，并进行打乱\n",
    "\n",
    "paddle.batch()表示每BATCH_SIZE组成一个batch\n",
    "\n",
    "（3）打印看下数据是什么样的？PaddlePaddle接口提供的数据已经经过了归一化、居中等处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaon\\AppData\\Local\\Temp/ipykernel_5152/4163987388.py:6: DeprecationWarning: \n",
      "Warning:\n",
      "API \"paddle.dataset.mnist.train\" is deprecated since 2.0.0, and will be removed in future versions. Please use \"paddle.vision.datasets.MNIST\" instead.\n",
      "reason: Please use new dataset API which supports paddle.io.DataLoader \n",
      "  paddle.reader.shuffle(paddle.dataset.mnist.train(),#从mnist数据集中获取数据\n",
      "Cache file C:\\Users\\Jaon\\.cache\\paddle\\dataset\\mnist\\train-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-images-idx3-ubyte.gz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n",
      "Cache file C:\\Users\\Jaon\\.cache\\paddle\\dataset\\mnist\\train-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-labels-idx1-ubyte.gz \n",
      "Begin to download\n",
      "........\n",
      "Download finished\n",
      "C:\\Users\\Jaon\\AppData\\Local\\Temp/ipykernel_5152/4163987388.py:11: DeprecationWarning: \n",
      "Warning:\n",
      "API \"paddle.dataset.mnist.test\" is deprecated since 2.0.0, and will be removed in future versions. Please use \"paddle.vision.datasets.MNIST\" instead.\n",
      "reason: Please use new dataset API which supports paddle.io.DataLoader \n",
      "  paddle.reader.shuffle(paddle.dataset.mnist.test(),\n",
      "Cache file C:\\Users\\Jaon\\.cache\\paddle\\dataset\\mnist\\t10k-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-images-idx3-ubyte.gz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n",
      "Cache file C:\\Users\\Jaon\\.cache\\paddle\\dataset\\mnist\\t10k-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-labels-idx1-ubyte.gz \n",
      "Begin to download\n",
      "..\n",
      "Download finished\n"
     ]
    }
   ],
   "source": [
    "#用于训练的数据提供器，每次从缓存中随机读取批次大小的数据\n",
    "BUF_SIZE=512\n",
    "BATCH_SIZE=128\n",
    "#用于训练的数据提供器，每次从缓存中随机读取批次大小的数据\n",
    "train_reader = paddle.batch(\n",
    "    paddle.reader.shuffle(paddle.dataset.mnist.train(),#从mnist数据集中获取数据\n",
    "                          buf_size=BUF_SIZE),#缓存为512条\n",
    "    batch_size=BATCH_SIZE)#每个batch为128条，一共是四个batch\n",
    "#用于训练的数据提供器，每次从缓存中随机读取批次大小的数据\n",
    "test_reader = paddle.batch(\n",
    "    paddle.reader.shuffle(paddle.dataset.mnist.test(),\n",
    "                          buf_size=BUF_SIZE),#首先缓存过来，再以batch的方式获取数据\n",
    "    batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Step2.网络配置**\n",
    "以下的代码判断就是定义一个简单的多层感知器，一共有三层，两个大小为100的隐层和一个大小为10的输出层，因为MNIST数据集是手写0到9的灰度图像，类别有10个，所以最后的输出大小是10。最后输出层的激活函数是Softmax，所以最后的输出层相当于一个分类器。加上一个输入层的话，多层感知器的结构是：输入层-->>隐层-->>隐层-->>输出层。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cb69f928778c4299b75814179607a89eea770bdc409d4e08a87e2975cb96b19b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(input):\n",
    "    hidden1 = paddle.fluid.layers.fc(input=input,size=100,act='relu')\n",
    "\n",
    "    hidden2 = paddle.fluid.layers.fc(input=hidden1,size=100,act='relu')\n",
    "\n",
    "    prediction = paddle.fluid.layers.fc(input=hidden2,size=10,act='softmax')\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "（2）定义数据层\n",
    "\n",
    "输入的是图像数据。图像是 28 * 28 的灰度图，所以输入的形状是[1, 28, 28]，如果图像是32*32的彩色图，那么输入的形状是[3. 32, 32]，因为灰度图只有一个通道，而彩色图有RGB三个通道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = paddle.fluid.data(name='image',shape=[1,28,28],dtype='float32')\n",
    "\n",
    "label = paddle.fluid.data(namae='label',shape=[1],dtype='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "（3）获取分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取分类器\n",
    "predict = multilayer_perceptron(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "（4）定义损失函数和准确率\n",
    "\n",
    "这次使用的是交叉熵损失函数，该函数在分类任务上比较常用。\n",
    "\n",
    "定义了一个损失函数之后，还有对它求平均值，训练程序必须返回平均损失作为第一个返回值，因为它会被后面反向传播算法所用到。。\n",
    "\n",
    "同时我们还可以定义一个准确率函数，这个可以在我们训练的时候输出分类的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#使用交叉熵损失函数,描述真实样本标签和预测概率之间的差值\n",
    "cost = paddle.layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "（5）定义优化函数\n",
    "\n",
    "这次我们使用的是Adam优化方法，同时指定学习率为0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #使用Adam算法进行优化, learning_rate 是学习率(它的大小与网络的训练收敛速度有关系)\n",
    "optimizer = fluid.optimizer.AdamOptimizer(learning_rate=0.001)  \n",
    "opts = optimizer.minimize(avg_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在上述模型配置完毕后，得到两个fluid.Program：fluid.default_startup_program() 与fluid.default_main_program() 配置完毕了。\n",
    "\n",
    "参数初始化操作会被写入fluid.default_startup_program()\n",
    "\n",
    "fluid.default_main_program()用于获取默认或全局main program(主程序)。该主程序用于训练和测试模型。fluid.layers 中的所有layer函数可以向 default_main_program 中添加算子和变量。default_main_program 是fluid的许多编程接口（API）的Program参数的缺省值。例如,当用户program没有传入的时候， Executor.run() 会默认执行 default_main_program 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Step3.模型训练 and Step4.模型评估**\n",
    "（1）创建训练的Executor\n",
    "\n",
    "首先定义运算场所 fluid.CPUPlace()和 fluid.CUDAPlace(0)分别表示运算场所为CPU和GPU\n",
    "\n",
    "Executor:接收传入的program，通过run()方法运行program。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义使用CPU还是GPU，使用CPU时use_cuda = False,使用GPU时use_cuda = True\n",
    "use_cuda = True\n",
    "place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n",
    "# 获取测试程序\n",
    "test_program = fluid.default_main_program().clone(for_test=True)\n",
    "exe = fluid.Executor(place)\n",
    "exe.run(fluid.default_startup_program())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "（2）告知网络传入的数据分为两部分，第一部分是image值，第二部分是label值\n",
    "\n",
    "DataFeeder负责将数据提供器（train_reader,test_reader）返回的数据转成一种特殊的数据结构，使其可以输入到Executor中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "feeder = fluid.DataFeeder(place=place, feed_list=[image, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "(3)展示模型训练曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_iter=0\n",
    "all_train_iters=[]\n",
    "all_train_costs=[]\n",
    "all_train_accs=[]\n",
    "\n",
    "def draw_train_process(title,iters,costs,accs,label_cost,lable_acc):\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"iter\", fontsize=20)\n",
    "    plt.ylabel(\"cost/acc\", fontsize=20)\n",
    "    plt.plot(iters, costs,color='red',label=label_cost) \n",
    "    plt.plot(iters, accs,color='green',label=lable_acc) \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "（4）训练并保存模型\n",
    "\n",
    "训练需要有一个训练程序和一些必要参数，并构建了一个获取训练过程中测试误差的函数。必要参数有executor,program,reader,feeder,fetch_list。\n",
    "\n",
    "**executor**表示之前创建的执行器\n",
    "\n",
    "**program**表示执行器所执行的program，是之前创建的program，如果该项参数没有给定的话则默认使用defalut_main_program\n",
    "\n",
    "**reader**表示读取到的数据\n",
    "\n",
    "**feeder**表示前向输入的变量\n",
    "\n",
    "**fetch_list**表示用户想得到的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EPOCH_NUM=2\n",
    "model_save_dir = \"/home/aistudio/work/hand.inference.model\"\n",
    "for pass_id in range(EPOCH_NUM):\n",
    "    # 进行训练\n",
    "    for batch_id, data in enumerate(train_reader()):                         #遍历train_reader\n",
    "        train_cost, train_acc = exe.run(program=fluid.default_main_program(),#运行主程序\n",
    "                                        feed=feeder.feed(data),              #给模型喂入数据\n",
    "                                        fetch_list=[avg_cost, acc])          #fetch 误差、准确率  \n",
    "        \n",
    "        all_train_iter=all_train_iter+BATCH_SIZE\n",
    "        all_train_iters.append(all_train_iter)\n",
    "        #print(train_acc)\n",
    "        all_train_costs.append(train_cost[0])\n",
    "        all_train_accs.append(train_acc[0])\n",
    "\n",
    "        # 每200个batch打印一次信息  误差、准确率\n",
    "        if batch_id % 200 == 0:\n",
    "            print('Pass:%d, Batch:%d, Cost:%0.5f, Accuracy:%0.5f' %\n",
    "                  (pass_id, batch_id, train_cost[0], train_acc[0]))\n",
    "\n",
    "    # 进行测试\n",
    "    test_accs = []\n",
    "    test_costs = []\n",
    "    #每训练一轮 进行一次测试\n",
    "    for batch_id, data in enumerate(test_reader()):                         #遍历test_reader\n",
    "        test_cost, test_acc = exe.run(program=test_program, #执行训练程序\n",
    "                                      feed=feeder.feed(data),               #喂入数据\n",
    "                                      fetch_list=[avg_cost, acc])           #fetch 误差、准确率\n",
    "        test_accs.append(test_acc[0])                                       #每个batch的准确率\n",
    "        test_costs.append(test_cost[0])                                     #每个batch的误差\n",
    "        \n",
    "       \n",
    "    # 求测试结果的平均值\n",
    "    test_cost = (sum(test_costs) / len(test_costs))                         #每轮的平均误差\n",
    "    test_acc = (sum(test_accs) / len(test_accs))                            #每轮的平均准确率\n",
    "    print('Test:%d, Cost:%0.5f, Accuracy:%0.5f' % (pass_id, test_cost, test_acc))\n",
    "    \n",
    "    #保存模型\n",
    "    # 如果保存路径不存在就创建\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "print ('save models to %s' % (model_save_dir))\n",
    "fluid.io.save_inference_model(model_save_dir,   #保存推理model的路径\n",
    "                                  ['image'],    #推理（inference）需要 feed 的数据\n",
    "                                  [predict],    #保存推理（inference）结果的 Variables\n",
    "                                  exe)             #executor 保存 inference model\n",
    "\n",
    "print('训练模型保存完成！')\n",
    "draw_train_process(\"training\",all_train_iters,all_train_costs,all_train_accs,\"trainning cost\",\"trainning acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **Step5.模型预测**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "（1）图片预处理\n",
    "\n",
    "在预测之前，要对图像进行预处理。\n",
    "\n",
    "首先进行灰度化，然后压缩图像大小为28*28，接着将图像转换成一维向量，最后再对一维向量进行归一化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_image(file):\n",
    "    im = Image.open(file).convert('L')                        #将RGB转化为灰度图像，L代表灰度图像，像素值在0~255之间\n",
    "    im = im.resize((28, 28), Image.ANTIALIAS)                 #resize image with high-quality 图像大小为28*28，抗锯齿\n",
    "    print(\"原始图片\",np.array(im).shape)\n",
    "    im = np.array(im).reshape(1, 1, 28, 28).astype(np.float32)#返回新形状的数组,把它变成一个 numpy 数组以匹配数据馈送格式。\n",
    "    print(\"处理图片\",im.shape)\n",
    "    im = im / 255.0 * 2.0 - 1.0                               #归一化到【-1~1】之间\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "（2）使用Matplotlib工具显示这张图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infer_path='/home/aistudio/data/data1910/infer_3.png'\n",
    "img = Image.open(infer_path)\n",
    "plt.imshow(img)   #根据数组绘制图像\n",
    "plt.show()        #显示图像\n",
    "load_image(infer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "(3)创建预测用的Executer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infer_exe = fluid.Executor(place)\n",
    "inference_scope = fluid.core.Scope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "(4)开始预测\n",
    "\n",
    "通过fluid.io.load_inference_model，预测器会从params_dirname中读取已经训练好的模型，来对从未遇见过的数据进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 加载数据并开始预测\n",
    "with fluid.scope_guard(inference_scope):\n",
    "    #获取训练好的模型\n",
    "    #从指定目录中加载 推理model(inference model)\n",
    "    [inference_program,                                            #推理Program\n",
    "     feed_target_names,                                            #是一个str列表，它包含需要在推理 Program 中提供数据的变量的名称。 \n",
    "     fetch_targets] = fluid.io.load_inference_model(model_save_dir,#fetch_targets：是一个 Variable 列表，从中我们可以得到推断结果。model_save_dir：模型保存的路径\n",
    "                                                    infer_exe)     #infer_exe: 运行 inference model的 executor\n",
    "    img = load_image(infer_path)\n",
    "\n",
    "    # 获取概率最大的label\n",
    "    lab = np.argsort(results)                                  #argsort函数返回的是result数组值从小到大的索引值\n",
    "    print(lab)\n",
    "    print(\"该图片的预测结果的label为: %d\" % lab[0][0][-1])     #-1代表读取数组中倒数第一列  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73fe03891eabd6446a090ee4c22d8dcac73542d36d6aee5d53443fd4b857a5ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
