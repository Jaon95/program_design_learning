{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.io import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import paddle.io\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据path\n",
    "data_path = 'data/news_classify_data.txt'\n",
    "dic_path = 'data/dict_txt.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载字典\n",
    "label2id = {}\n",
    "with open(dic_path,'r',encoding='utf-8') as f:\n",
    "    label2id = eval(f.read())\n",
    "id2label = [(wordid,label) for label,wordid in label2id.items() ]\n",
    "id2label = dict(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有label\n",
    "names = [ '文化', '娱乐', '体育', '财经','房产', '汽车', '教育', '科技', '国际', '证券']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Tokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self,label2id) -> None:\n",
    "        self.label2id = label2id\n",
    "        id2label = [(wordid,label) for label,wordid in label2id.items() ]\n",
    "        self.id2label = dict(id2label)\n",
    "    \n",
    "    def encode(self,text,max_len):\n",
    "        encode = []\n",
    "        for c in text:\n",
    "            encode.append(self.label2id.get(c,0))\n",
    "        encode = encode[0:min(len(encode),max_len)]\n",
    "        if len(encode) < max_len:\n",
    "            m = [self.pad_id for i in range(max_len-len(encode))]\n",
    "            encode.extend(m)\n",
    "        return encode\n",
    "    \n",
    "    def decode(self,encode):\n",
    "        decode = []\n",
    "        for i in encode:\n",
    "            decode.append(self.id2label.get(i,'u'))\n",
    "        return ''.join(decode)\n",
    "\n",
    "    @property\n",
    "    def voc_size(self):\n",
    "        return len(self.label2id)\n",
    "    \n",
    "    @property\n",
    "    def pad_id(self):\n",
    "        return self.label2id['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义转换函数\n",
    "def convert_sample(data,tokenizer,is_predict = False,max_len = max_seq_len):\n",
    "    encode = tokenizer.encode(data['text'],max_len)\n",
    "    label = int(data['label'])\n",
    "    if not is_predict:\n",
    "        return np.array(encode).astype(np.int64),np.int64(label)\n",
    "    else:\n",
    "        return np.array(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(label2id)\n",
    "sample_trans = partial(convert_sample,tokenizer = tokenizer,is_predict = False,max_len  = max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "\n",
    "    def load_data(self,data_path):\n",
    "        with open(data_path,'r',encoding='utf-8') as f:\n",
    "            lines = [line.strip().split('_!_') for line in f.readlines()]\n",
    "            data = [{'text':text,'label':label} for _,label,_,text in lines]\n",
    "        temp = []\n",
    "        for item in data:\n",
    "            temp.append(sample_trans(item))\n",
    "        return temp\n",
    "\n",
    "    def __init__(self,data_path):\n",
    "        self.dataset = self.load_data(data_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ClassifierDataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(dataset,batch_size = 1,mode = 'train'):\n",
    "    shuffle = True if mode == 'train' else False\n",
    "\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset=dataset,batch_size=batch_size,shuffle=shuffle\n",
    "        )\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset=dataset,batch_size=batch_size,shuffle=shuffle\n",
    "        )\n",
    "\n",
    "    data_loader = paddle.io.DataLoader(dataset,batch_sampler=batch_sampler,return_list=True)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义网络结构的超参数\n",
    "voc_size = tokenizer.voc_size #\n",
    "embed_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self,voc_size,embed_size,classes):\n",
    "        super(TextClassifier,self).__init__()\n",
    "\n",
    "        # 创建词向量\n",
    "        self.emb = paddle.nn.Embedding(num_embeddings=voc_size,embedding_dim=embed_size)\n",
    "\n",
    "        # 卷积层\n",
    "        self.conv1 = paddle.nn.Conv1D(in_channels=embed_size,out_channels=20,kernel_size=5,stride=2,data_format='NLC')\n",
    "        # pool层\n",
    "        self.maxpool1 = paddle.nn.MaxPool1D(kernel_size=2,stride=2)\n",
    "\n",
    "        # 卷积层\n",
    "        self.conv2 = paddle.nn.Conv1D(in_channels=embed_size,out_channels=30,kernel_size=5,stride=2,data_format='NLC')\n",
    "        # pool层\n",
    "        self.maxpool2 = paddle.nn.MaxPool1D(kernel_size=2,stride=2)\n",
    "\n",
    "        self.linear = paddle.nn.Linear(in_features=1025,out_features=len(classes))\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        emb1 = self.emb(inputs)\n",
    "        \n",
    "        con1 = self.conv1(emb1)\n",
    "        pool1 = self.maxpool1(con1)\n",
    "\n",
    "        con2 = self.conv2(emb1)\n",
    "        pool2 = self.maxpool2(con2)\n",
    "\n",
    "        line_inputs = paddle.concat([pool1,pool2],axis=2).reshape([-1,1025])\n",
    "        # line_inputs = pool2.reshape([-1,615])\n",
    "        outputs = self.linear(line_inputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "textClf = TextClassifier(voc_size=voc_size,embed_size=embed_size,classes=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      " Embedding-17        [[1, 85]]           [1, 85, 128]         606,080    \n",
      "   Conv1D-29       [[1, 85, 128]]        [1, 41, 20]          12,820     \n",
      " MaxPool1D-29      [[1, 41, 20]]         [1, 41, 10]             0       \n",
      "   Conv1D-30       [[1, 85, 128]]        [1, 41, 30]          19,230     \n",
      " MaxPool1D-30      [[1, 41, 30]]         [1, 41, 15]             0       \n",
      "   Linear-17        [[1, 1025]]            [1, 10]            10,260     \n",
      "===========================================================================\n",
      "Total params: 648,390\n",
      "Trainable params: 648,390\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 2.47\n",
      "Estimated Total Size (MB): 2.58\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 648390, 'trainable_params': 648390}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddle.summary(textClf,input_size=(-1,85),dtypes='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.optimizer\n",
    "import paddle.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 5\n",
    "batch_size = 125\n",
    "train_data = data_loader(train_set,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 0, batch: 10, loss: 1.01823, acc: 0.59040\n",
      "global step 20, epoch: 0, batch: 20, loss: 1.04618, acc: 0.62560\n",
      "global step 30, epoch: 0, batch: 30, loss: 1.09615, acc: 0.63893\n",
      "global step 40, epoch: 0, batch: 40, loss: 0.97268, acc: 0.65400\n",
      "global step 50, epoch: 0, batch: 50, loss: 0.89807, acc: 0.66224\n",
      "global step 60, epoch: 0, batch: 60, loss: 0.89844, acc: 0.66800\n",
      "global step 70, epoch: 0, batch: 70, loss: 1.18668, acc: 0.67154\n",
      "global step 80, epoch: 0, batch: 80, loss: 0.93560, acc: 0.67760\n",
      "global step 90, epoch: 0, batch: 90, loss: 0.83602, acc: 0.68400\n",
      "global step 100, epoch: 0, batch: 100, loss: 0.87337, acc: 0.68528\n",
      "global step 110, epoch: 0, batch: 110, loss: 0.74424, acc: 0.68829\n",
      "global step 120, epoch: 0, batch: 120, loss: 0.95913, acc: 0.68860\n",
      "global step 130, epoch: 0, batch: 130, loss: 0.81763, acc: 0.69280\n",
      "global step 140, epoch: 0, batch: 140, loss: 0.85846, acc: 0.69583\n",
      "global step 150, epoch: 0, batch: 150, loss: 0.73787, acc: 0.70000\n",
      "global step 160, epoch: 0, batch: 160, loss: 0.71627, acc: 0.70290\n",
      "global step 170, epoch: 0, batch: 170, loss: 0.72370, acc: 0.70546\n",
      "global step 180, epoch: 0, batch: 180, loss: 0.86681, acc: 0.70627\n",
      "global step 190, epoch: 0, batch: 190, loss: 0.84466, acc: 0.70737\n",
      "global step 200, epoch: 0, batch: 200, loss: 0.72278, acc: 0.70840\n",
      "global step 210, epoch: 0, batch: 210, loss: 0.81144, acc: 0.70888\n",
      "global step 220, epoch: 0, batch: 220, loss: 0.76614, acc: 0.71015\n",
      "global step 230, epoch: 0, batch: 230, loss: 0.92245, acc: 0.71075\n",
      "global step 240, epoch: 0, batch: 240, loss: 0.64265, acc: 0.71190\n",
      "global step 250, epoch: 0, batch: 250, loss: 0.79608, acc: 0.71245\n",
      "global step 260, epoch: 0, batch: 260, loss: 0.67882, acc: 0.71378\n",
      "global step 270, epoch: 0, batch: 270, loss: 0.77980, acc: 0.71544\n",
      "global step 280, epoch: 0, batch: 280, loss: 0.68132, acc: 0.71666\n",
      "global step 290, epoch: 0, batch: 290, loss: 0.67564, acc: 0.71757\n",
      "global step 300, epoch: 0, batch: 300, loss: 0.65899, acc: 0.71883\n",
      "global step 310, epoch: 0, batch: 310, loss: 0.77346, acc: 0.71917\n",
      "global step 320, epoch: 0, batch: 320, loss: 0.61096, acc: 0.72025\n",
      "global step 330, epoch: 0, batch: 330, loss: 0.76084, acc: 0.72080\n",
      "global step 340, epoch: 0, batch: 340, loss: 0.68535, acc: 0.72191\n",
      "global step 350, epoch: 0, batch: 350, loss: 0.81258, acc: 0.72297\n",
      "global step 360, epoch: 0, batch: 360, loss: 0.72141, acc: 0.72340\n",
      "global step 370, epoch: 0, batch: 370, loss: 0.83945, acc: 0.72400\n",
      "global step 380, epoch: 0, batch: 380, loss: 0.72673, acc: 0.72484\n",
      "global step 390, epoch: 0, batch: 390, loss: 0.90222, acc: 0.72570\n",
      "global step 400, epoch: 0, batch: 400, loss: 0.76477, acc: 0.72624\n",
      "global step 410, epoch: 0, batch: 410, loss: 0.83473, acc: 0.72699\n",
      "global step 420, epoch: 0, batch: 420, loss: 0.73116, acc: 0.72752\n",
      "global step 430, epoch: 0, batch: 430, loss: 0.78291, acc: 0.72867\n",
      "global step 440, epoch: 0, batch: 440, loss: 0.98706, acc: 0.72902\n",
      "global step 450, epoch: 0, batch: 450, loss: 0.83042, acc: 0.72994\n",
      "global step 460, epoch: 1, batch: 5, loss: 0.56194, acc: 0.73096\n",
      "global step 470, epoch: 1, batch: 15, loss: 0.68110, acc: 0.73216\n",
      "global step 480, epoch: 1, batch: 25, loss: 0.79414, acc: 0.73319\n",
      "global step 490, epoch: 1, batch: 35, loss: 0.74797, acc: 0.73466\n",
      "global step 500, epoch: 1, batch: 45, loss: 0.55907, acc: 0.73603\n",
      "global step 510, epoch: 1, batch: 55, loss: 0.61461, acc: 0.73706\n",
      "global step 520, epoch: 1, batch: 65, loss: 0.62208, acc: 0.73794\n",
      "global step 530, epoch: 1, batch: 75, loss: 0.70470, acc: 0.73888\n",
      "global step 540, epoch: 1, batch: 85, loss: 0.68751, acc: 0.73993\n",
      "global step 550, epoch: 1, batch: 95, loss: 0.78763, acc: 0.74097\n",
      "global step 560, epoch: 1, batch: 105, loss: 0.59081, acc: 0.74226\n",
      "global step 570, epoch: 1, batch: 115, loss: 0.74049, acc: 0.74345\n",
      "global step 580, epoch: 1, batch: 125, loss: 0.68806, acc: 0.74411\n",
      "global step 590, epoch: 1, batch: 135, loss: 0.64449, acc: 0.74484\n",
      "global step 600, epoch: 1, batch: 145, loss: 0.45892, acc: 0.74606\n",
      "global step 610, epoch: 1, batch: 155, loss: 0.68610, acc: 0.74688\n",
      "global step 620, epoch: 1, batch: 165, loss: 0.70990, acc: 0.74736\n",
      "global step 630, epoch: 1, batch: 175, loss: 0.56691, acc: 0.74799\n",
      "global step 640, epoch: 1, batch: 185, loss: 0.68960, acc: 0.74899\n",
      "global step 650, epoch: 1, batch: 195, loss: 0.82774, acc: 0.74933\n",
      "global step 660, epoch: 1, batch: 205, loss: 0.68507, acc: 0.74990\n",
      "global step 670, epoch: 1, batch: 215, loss: 0.55535, acc: 0.75072\n",
      "global step 680, epoch: 1, batch: 225, loss: 0.62948, acc: 0.75124\n",
      "global step 690, epoch: 1, batch: 235, loss: 0.67341, acc: 0.75164\n",
      "global step 700, epoch: 1, batch: 245, loss: 0.60302, acc: 0.75236\n",
      "global step 710, epoch: 1, batch: 255, loss: 0.74583, acc: 0.75268\n",
      "global step 720, epoch: 1, batch: 265, loss: 0.61578, acc: 0.75326\n",
      "global step 730, epoch: 1, batch: 275, loss: 0.60227, acc: 0.75394\n",
      "global step 740, epoch: 1, batch: 285, loss: 0.66379, acc: 0.75467\n",
      "global step 750, epoch: 1, batch: 295, loss: 0.70176, acc: 0.75530\n",
      "global step 760, epoch: 1, batch: 305, loss: 0.68006, acc: 0.75577\n",
      "global step 770, epoch: 1, batch: 315, loss: 0.65695, acc: 0.75620\n",
      "global step 780, epoch: 1, batch: 325, loss: 0.59383, acc: 0.75658\n",
      "global step 790, epoch: 1, batch: 335, loss: 0.69375, acc: 0.75711\n",
      "global step 800, epoch: 1, batch: 345, loss: 0.76948, acc: 0.75749\n",
      "global step 810, epoch: 1, batch: 355, loss: 0.73307, acc: 0.75793\n",
      "global step 820, epoch: 1, batch: 365, loss: 0.63862, acc: 0.75830\n",
      "global step 830, epoch: 1, batch: 375, loss: 0.52921, acc: 0.75871\n",
      "global step 840, epoch: 1, batch: 385, loss: 0.56572, acc: 0.75903\n",
      "global step 850, epoch: 1, batch: 395, loss: 0.64202, acc: 0.75949\n",
      "global step 860, epoch: 1, batch: 405, loss: 0.79067, acc: 0.75986\n",
      "global step 870, epoch: 1, batch: 415, loss: 0.58388, acc: 0.76028\n",
      "global step 880, epoch: 1, batch: 425, loss: 0.68954, acc: 0.76062\n",
      "global step 890, epoch: 1, batch: 435, loss: 0.61786, acc: 0.76112\n",
      "global step 900, epoch: 1, batch: 445, loss: 0.82170, acc: 0.76137\n",
      "global step 910, epoch: 1, batch: 455, loss: 0.53411, acc: 0.76172\n",
      "global step 920, epoch: 2, batch: 10, loss: 0.58154, acc: 0.76238\n",
      "global step 930, epoch: 2, batch: 20, loss: 0.57050, acc: 0.76299\n",
      "global step 940, epoch: 2, batch: 30, loss: 0.55826, acc: 0.76353\n",
      "global step 950, epoch: 2, batch: 40, loss: 0.71997, acc: 0.76422\n",
      "global step 960, epoch: 2, batch: 50, loss: 0.53382, acc: 0.76466\n",
      "global step 970, epoch: 2, batch: 60, loss: 0.49762, acc: 0.76515\n",
      "global step 980, epoch: 2, batch: 70, loss: 0.58924, acc: 0.76568\n",
      "global step 990, epoch: 2, batch: 80, loss: 0.56063, acc: 0.76618\n",
      "global step 1000, epoch: 2, batch: 90, loss: 0.69383, acc: 0.76663\n",
      "global step 1010, epoch: 2, batch: 100, loss: 0.51092, acc: 0.76716\n",
      "global step 1020, epoch: 2, batch: 110, loss: 0.42577, acc: 0.76769\n",
      "global step 1030, epoch: 2, batch: 120, loss: 0.49238, acc: 0.76819\n",
      "global step 1040, epoch: 2, batch: 130, loss: 0.73020, acc: 0.76860\n",
      "global step 1050, epoch: 2, batch: 140, loss: 0.62624, acc: 0.76888\n",
      "global step 1060, epoch: 2, batch: 150, loss: 0.65966, acc: 0.76909\n",
      "global step 1070, epoch: 2, batch: 160, loss: 0.59742, acc: 0.76959\n",
      "global step 1080, epoch: 2, batch: 170, loss: 0.57954, acc: 0.77004\n",
      "global step 1090, epoch: 2, batch: 180, loss: 0.43591, acc: 0.77066\n",
      "global step 1100, epoch: 2, batch: 190, loss: 0.54188, acc: 0.77118\n",
      "global step 1110, epoch: 2, batch: 200, loss: 0.80051, acc: 0.77147\n",
      "global step 1120, epoch: 2, batch: 210, loss: 0.59168, acc: 0.77197\n",
      "global step 1130, epoch: 2, batch: 220, loss: 0.63626, acc: 0.77229\n",
      "global step 1140, epoch: 2, batch: 230, loss: 0.52377, acc: 0.77266\n",
      "global step 1150, epoch: 2, batch: 240, loss: 0.71180, acc: 0.77303\n",
      "global step 1160, epoch: 2, batch: 250, loss: 0.57773, acc: 0.77349\n",
      "global step 1170, epoch: 2, batch: 260, loss: 0.68583, acc: 0.77375\n",
      "global step 1180, epoch: 2, batch: 270, loss: 0.72349, acc: 0.77418\n",
      "global step 1190, epoch: 2, batch: 280, loss: 0.48967, acc: 0.77447\n",
      "global step 1200, epoch: 2, batch: 290, loss: 0.61814, acc: 0.77489\n",
      "global step 1210, epoch: 2, batch: 300, loss: 0.56960, acc: 0.77520\n",
      "global step 1220, epoch: 2, batch: 310, loss: 0.44010, acc: 0.77553\n",
      "global step 1230, epoch: 2, batch: 320, loss: 0.53157, acc: 0.77587\n",
      "global step 1240, epoch: 2, batch: 330, loss: 0.53982, acc: 0.77613\n",
      "global step 1250, epoch: 2, batch: 340, loss: 0.79123, acc: 0.77634\n",
      "global step 1260, epoch: 2, batch: 350, loss: 0.54985, acc: 0.77644\n",
      "global step 1270, epoch: 2, batch: 360, loss: 0.47830, acc: 0.77675\n",
      "global step 1280, epoch: 2, batch: 370, loss: 0.58919, acc: 0.77710\n",
      "global step 1290, epoch: 2, batch: 380, loss: 0.54102, acc: 0.77747\n",
      "global step 1300, epoch: 2, batch: 390, loss: 0.61014, acc: 0.77778\n",
      "global step 1310, epoch: 2, batch: 400, loss: 0.57370, acc: 0.77791\n",
      "global step 1320, epoch: 2, batch: 410, loss: 0.53645, acc: 0.77813\n",
      "global step 1330, epoch: 2, batch: 420, loss: 0.53473, acc: 0.77847\n",
      "global step 1340, epoch: 2, batch: 430, loss: 0.67026, acc: 0.77868\n",
      "global step 1350, epoch: 2, batch: 440, loss: 0.72739, acc: 0.77892\n",
      "global step 1360, epoch: 2, batch: 450, loss: 0.67397, acc: 0.77907\n",
      "global step 1370, epoch: 3, batch: 5, loss: 0.48799, acc: 0.77935\n",
      "global step 1380, epoch: 3, batch: 15, loss: 0.49023, acc: 0.77979\n",
      "global step 1390, epoch: 3, batch: 25, loss: 0.64018, acc: 0.78008\n",
      "global step 1400, epoch: 3, batch: 35, loss: 0.50443, acc: 0.78058\n",
      "global step 1410, epoch: 3, batch: 45, loss: 0.49909, acc: 0.78093\n",
      "global step 1420, epoch: 3, batch: 55, loss: 0.56265, acc: 0.78129\n",
      "global step 1430, epoch: 3, batch: 65, loss: 0.62222, acc: 0.78153\n",
      "global step 1440, epoch: 3, batch: 75, loss: 0.59592, acc: 0.78183\n",
      "global step 1450, epoch: 3, batch: 85, loss: 0.54669, acc: 0.78220\n",
      "global step 1460, epoch: 3, batch: 95, loss: 0.44172, acc: 0.78260\n",
      "global step 1470, epoch: 3, batch: 105, loss: 0.36858, acc: 0.78305\n",
      "global step 1480, epoch: 3, batch: 115, loss: 0.58850, acc: 0.78331\n",
      "global step 1490, epoch: 3, batch: 125, loss: 0.59669, acc: 0.78367\n",
      "global step 1500, epoch: 3, batch: 135, loss: 0.56599, acc: 0.78387\n",
      "global step 1510, epoch: 3, batch: 145, loss: 0.53319, acc: 0.78419\n",
      "global step 1520, epoch: 3, batch: 155, loss: 0.59625, acc: 0.78444\n",
      "global step 1530, epoch: 3, batch: 165, loss: 0.46896, acc: 0.78482\n",
      "global step 1540, epoch: 3, batch: 175, loss: 0.39056, acc: 0.78520\n",
      "global step 1550, epoch: 3, batch: 185, loss: 0.50895, acc: 0.78561\n",
      "global step 1560, epoch: 3, batch: 195, loss: 0.44338, acc: 0.78587\n",
      "global step 1570, epoch: 3, batch: 205, loss: 0.57075, acc: 0.78613\n",
      "global step 1580, epoch: 3, batch: 215, loss: 0.42501, acc: 0.78647\n",
      "global step 1590, epoch: 3, batch: 225, loss: 0.44487, acc: 0.78675\n",
      "global step 1600, epoch: 3, batch: 235, loss: 0.61494, acc: 0.78701\n",
      "global step 1610, epoch: 3, batch: 245, loss: 0.56509, acc: 0.78709\n",
      "global step 1620, epoch: 3, batch: 255, loss: 0.56607, acc: 0.78740\n",
      "global step 1630, epoch: 3, batch: 265, loss: 0.47516, acc: 0.78759\n",
      "global step 1640, epoch: 3, batch: 275, loss: 0.49989, acc: 0.78780\n",
      "global step 1650, epoch: 3, batch: 285, loss: 0.43009, acc: 0.78797\n",
      "global step 1660, epoch: 3, batch: 295, loss: 0.54222, acc: 0.78813\n",
      "global step 1670, epoch: 3, batch: 305, loss: 0.48317, acc: 0.78836\n",
      "global step 1680, epoch: 3, batch: 315, loss: 0.39826, acc: 0.78853\n",
      "global step 1690, epoch: 3, batch: 325, loss: 0.55393, acc: 0.78866\n",
      "global step 1700, epoch: 3, batch: 335, loss: 0.58651, acc: 0.78888\n",
      "global step 1710, epoch: 3, batch: 345, loss: 0.54198, acc: 0.78899\n",
      "global step 1720, epoch: 3, batch: 355, loss: 0.41435, acc: 0.78926\n",
      "global step 1730, epoch: 3, batch: 365, loss: 0.61583, acc: 0.78946\n",
      "global step 1740, epoch: 3, batch: 375, loss: 0.38204, acc: 0.78972\n",
      "global step 1750, epoch: 3, batch: 385, loss: 0.49335, acc: 0.78995\n",
      "global step 1760, epoch: 3, batch: 395, loss: 0.66427, acc: 0.79009\n",
      "global step 1770, epoch: 3, batch: 405, loss: 0.51616, acc: 0.79038\n",
      "global step 1780, epoch: 3, batch: 415, loss: 0.61247, acc: 0.79058\n",
      "global step 1790, epoch: 3, batch: 425, loss: 0.49654, acc: 0.79086\n",
      "global step 1800, epoch: 3, batch: 435, loss: 0.45225, acc: 0.79113\n",
      "global step 1810, epoch: 3, batch: 445, loss: 0.41861, acc: 0.79139\n",
      "global step 1820, epoch: 3, batch: 455, loss: 0.39950, acc: 0.79160\n",
      "global step 1830, epoch: 4, batch: 10, loss: 0.56979, acc: 0.79187\n",
      "global step 1840, epoch: 4, batch: 20, loss: 0.48528, acc: 0.79217\n",
      "global step 1850, epoch: 4, batch: 30, loss: 0.45265, acc: 0.79255\n",
      "global step 1860, epoch: 4, batch: 40, loss: 0.38970, acc: 0.79280\n",
      "global step 1870, epoch: 4, batch: 50, loss: 0.37727, acc: 0.79313\n",
      "global step 1880, epoch: 4, batch: 60, loss: 0.40739, acc: 0.79350\n",
      "global step 1890, epoch: 4, batch: 70, loss: 0.54280, acc: 0.79370\n",
      "global step 1900, epoch: 4, batch: 80, loss: 0.34190, acc: 0.79401\n",
      "global step 1910, epoch: 4, batch: 90, loss: 0.54042, acc: 0.79423\n",
      "global step 1920, epoch: 4, batch: 100, loss: 0.47983, acc: 0.79453\n",
      "global step 1930, epoch: 4, batch: 110, loss: 0.60676, acc: 0.79480\n",
      "global step 1940, epoch: 4, batch: 120, loss: 0.51217, acc: 0.79505\n",
      "global step 1950, epoch: 4, batch: 130, loss: 0.55131, acc: 0.79527\n",
      "global step 1960, epoch: 4, batch: 140, loss: 0.51349, acc: 0.79549\n",
      "global step 1970, epoch: 4, batch: 150, loss: 0.44552, acc: 0.79567\n",
      "global step 1980, epoch: 4, batch: 160, loss: 0.46325, acc: 0.79591\n",
      "global step 1990, epoch: 4, batch: 170, loss: 0.56530, acc: 0.79612\n",
      "global step 2000, epoch: 4, batch: 180, loss: 0.54934, acc: 0.79633\n",
      "global step 2010, epoch: 4, batch: 190, loss: 0.51596, acc: 0.79655\n",
      "global step 2020, epoch: 4, batch: 200, loss: 0.53597, acc: 0.79674\n",
      "global step 2030, epoch: 4, batch: 210, loss: 0.57528, acc: 0.79694\n",
      "global step 2040, epoch: 4, batch: 220, loss: 0.53224, acc: 0.79711\n",
      "global step 2050, epoch: 4, batch: 230, loss: 0.40468, acc: 0.79729\n",
      "global step 2060, epoch: 4, batch: 240, loss: 0.50650, acc: 0.79748\n",
      "global step 2070, epoch: 4, batch: 250, loss: 0.47806, acc: 0.79771\n",
      "global step 2080, epoch: 4, batch: 260, loss: 0.55617, acc: 0.79802\n",
      "global step 2090, epoch: 4, batch: 270, loss: 0.62132, acc: 0.79816\n",
      "global step 2100, epoch: 4, batch: 280, loss: 0.36764, acc: 0.79850\n",
      "global step 2110, epoch: 4, batch: 290, loss: 0.45438, acc: 0.79875\n",
      "global step 2120, epoch: 4, batch: 300, loss: 0.40738, acc: 0.79899\n",
      "global step 2130, epoch: 4, batch: 310, loss: 0.59039, acc: 0.79922\n",
      "global step 2140, epoch: 4, batch: 320, loss: 0.41501, acc: 0.79943\n",
      "global step 2150, epoch: 4, batch: 330, loss: 0.37255, acc: 0.79968\n",
      "global step 2160, epoch: 4, batch: 340, loss: 0.61823, acc: 0.79988\n",
      "global step 2170, epoch: 4, batch: 350, loss: 0.43776, acc: 0.80007\n",
      "global step 2180, epoch: 4, batch: 360, loss: 0.38754, acc: 0.80021\n",
      "global step 2190, epoch: 4, batch: 370, loss: 0.62285, acc: 0.80028\n",
      "global step 2200, epoch: 4, batch: 380, loss: 0.38073, acc: 0.80048\n",
      "global step 2210, epoch: 4, batch: 390, loss: 0.55330, acc: 0.80069\n",
      "global step 2220, epoch: 4, batch: 400, loss: 0.74887, acc: 0.80082\n",
      "global step 2230, epoch: 4, batch: 410, loss: 0.74140, acc: 0.80093\n",
      "global step 2240, epoch: 4, batch: 420, loss: 0.48243, acc: 0.80104\n",
      "global step 2250, epoch: 4, batch: 430, loss: 0.49414, acc: 0.80125\n",
      "global step 2260, epoch: 4, batch: 440, loss: 0.54555, acc: 0.80140\n",
      "global step 2270, epoch: 4, batch: 450, loss: 0.49721, acc: 0.80162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义优化器\n",
    "optimizer = paddle.optimizer.Adagrad(learning_rate=0.01,parameters=textClf.parameters())\n",
    "# 定义统计指标\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "global_step = 0\n",
    "textClf.train()\n",
    "for epoch in range(epoch_num):\n",
    "    \n",
    "    for step,data in enumerate(train_data, start=1):\n",
    "        x_train,y_train = data\n",
    "        outputs = textClf(x_train)\n",
    "\n",
    "        prediction = F.softmax(outputs)\n",
    "        loss = F.cross_entropy(outputs,y_train)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "\n",
    "        # 使用softmax计算概率\n",
    "        probs = F.softmax(outputs,axis=1)\n",
    "        # 统计\n",
    "        correct = metric.compute(probs, y_train)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        # 打印中间训练结果\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "\n",
    "        # 参数更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73fe03891eabd6446a090ee4c22d8dcac73542d36d6aee5d53443fd4b857a5ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
